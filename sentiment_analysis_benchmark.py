#!/usr/bin/env python3
"""
Benchmark d'analyse de sentiments - Veille technologique Hugging Face
Auteur: [Votre nom]
Date: [Date]

Ce script compare 3 mod√®les d'analyse de sentiments disponibles sur Hugging Face
pour l'analyse automatique d'avis clients et commentaires.
"""

import time
import requests
import json
from transformers import pipeline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class SentimentAnalyzer:
    """Classe pour analyser les sentiments avec diff√©rents mod√®les Hugging Face"""
    
    def __init__(self):
        self.models = {
            'distilbert-english': {
                'name': 'distilbert-base-uncased-finetuned-sst-2-english',
                'languages': ['en'],
                'description': 'Mod√®le DistilBERT optimis√© pour l\'anglais',
                'expected_accuracy': '91.3%',
                'model_size_mb': 268,
                'api_endpoint': 'https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english'
            },
            'bert-multilingual': {
                'name': 'nlptown/bert-base-multilingual-uncased-sentiment',
                'languages': ['en', 'fr', 'de', 'es', 'it'],
                'description': 'Mod√®le BERT multilingue (5 √©toiles)',
                'expected_accuracy': '85.2%',
                'model_size_mb': 438,
                'api_endpoint': 'https://api-inference.huggingface.co/models/nlptown/bert-base-multilingual-uncased-sentiment'
            },
            'roberta-twitter': {
                'name': 'cardiffnlp/twitter-roberta-base-sentiment',
                'languages': ['en'],
                'description': 'RoBERTa optimis√© pour Twitter',
                'expected_accuracy': '89.1%',
                'model_size_mb': 499,
                'api_endpoint': 'https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment'
            }
        }
        
        self.test_samples = {
            'en': [
                "This product is absolutely amazing! I love it!",
                "The service was terrible and the staff was rude.",
                "It's okay, nothing special but works fine.",
                "Best purchase I've ever made, highly recommended!",
                "Disappointed with the quality, waste of money."
            ],
            'fr': [
                "Ce produit est absolument g√©nial ! Je l'adore !",
                "Le service √©tait terrible et le personnel √©tait impoli.",
                "C'est correct, rien de sp√©cial mais √ßa fonctionne bien.",
                "Meilleur achat que j'ai jamais fait, fortement recommand√© !",
                "D√©√ßu de la qualit√©, gaspillage d'argent."
            ]
        }
        
        self.results = {}
    
    def test_local_model(self, model_key: str) -> Dict:
        """Teste un mod√®le en local avec Transformers"""
        print(f"üîß Test du mod√®le local: {model_key}")
        
        model_info = self.models[model_key]
        results = {
            'model': model_key,
            'type': 'local',
            'responses': [],
            'response_times': [],
            'errors': []
        }
        
        try:
            # Chargement du mod√®le
            start_time = time.time()
            
            if model_key == 'bert-multilingual':
                # Mod√®le 5 √©toiles - conversion sp√©ciale
                classifier = pipeline('text-classification', model=model_info['name'])
                
                for lang, samples in self.test_samples.items():
                    if lang in model_info['languages']:
                        for sample in samples:
                            start = time.time()
                            try:
                                result = classifier(sample)
                                response_time = time.time() - start
                                
                                # Conversion du format 5 √©toiles vers sentiment
                                label = result[0]['label']
                                score = result[0]['score']
                                
                                # Conversion: 1-2 √©toiles = n√©gatif, 3 = neutre, 4-5 = positif
                                if '1 star' in label or '2 stars' in label:
                                    sentiment = 'NEGATIVE'
                                elif '3 stars' in label:
                                    sentiment = 'NEUTRAL'
                                else:
                                    sentiment = 'POSITIVE'
                                
                                results['responses'].append({
                                    'text': sample,
                                    'language': lang,
                                    'sentiment': sentiment,
                                    'confidence': score,
                                    'original_label': label
                                })
                                results['response_times'].append(response_time)
                                
                            except Exception as e:
                                results['errors'].append(f"Erreur sur '{sample}': {str(e)}")
                                
            else:
                # Mod√®les binaires (positif/n√©gatif)
                classifier = pipeline('sentiment-analysis', model=model_info['name'])
                
                for lang, samples in self.test_samples.items():
                    if lang in model_info['languages']:
                        for sample in samples:
                            start = time.time()
                            try:
                                result = classifier(sample)
                                response_time = time.time() - start
                                
                                results['responses'].append({
                                    'text': sample,
                                    'language': lang,
                                    'sentiment': result[0]['label'].upper(),
                                    'confidence': result[0]['score'],
                                    'original_label': result[0]['label']
                                })
                                results['response_times'].append(response_time)
                                
                            except Exception as e:
                                results['errors'].append(f"Erreur sur '{sample}': {str(e)}")
            
            load_time = time.time() - start_time
            results['load_time'] = load_time
            
        except Exception as e:
            results['errors'].append(f"Erreur de chargement: {str(e)}")
        
        return results
    
    def test_api_model(self, model_key: str) -> Dict:
        """Teste un mod√®le via l'API Hugging Face"""
        print(f"Test de l'API: {model_key}")
        
        model_info = self.models[model_key]
        results = {
            'model': model_key,
            'type': 'api',
            'responses': [],
            'response_times': [],
            'errors': []
        }
        
        for lang, samples in self.test_samples.items():
            if lang in model_info['languages']:
                for sample in samples:
                    start = time.time()
                    try:
                        response = requests.post(
                            model_info['api_endpoint'],
                            headers={"Content-Type": "application/json"},
                            json={"inputs": sample}
                        )
                        response_time = time.time() - start
                        
                        if response.status_code == 200:
                            result = response.json()
                            
                            if model_key == 'bert-multilingual':
                                # Traitement sp√©cial pour le mod√®le 5 √©toiles
                                label = result[0][0]['label']
                                score = result[0][0]['score']
                                
                                if '1 star' in label or '2 stars' in label:
                                    sentiment = 'NEGATIVE'
                                elif '3 stars' in label:
                                    sentiment = 'NEUTRAL'
                                else:
                                    sentiment = 'POSITIVE'
                            else:
                                label = result[0]['label']
                                score = result[0]['score']
                                sentiment = label.upper()
                            
                            results['responses'].append({
                                'text': sample,
                                'language': lang,
                                'sentiment': sentiment,
                                'confidence': score,
                                'original_label': label
                            })
                            results['response_times'].append(response_time)
                        else:
                            results['errors'].append(f"Erreur API {response.status_code}: {response.text}")
                            
                    except Exception as e:
                        results['errors'].append(f"Erreur sur '{sample}': {str(e)}")
        
        return results
    
    def run_benchmark(self, use_api: bool = False) -> Dict:
        """Lance le benchmark complet"""
        print("üöÄ D√©marrage du benchmark d'analyse de sentiments")
        print("=" * 60)
        
        all_results = {}
        
        for model_key in self.models.keys():
            print(f"\nüìä Test du mod√®le: {model_key}")
            print("-" * 40)
            
            if use_api:
                result = self.test_api_model(model_key)
            else:
                result = self.test_local_model(model_key)
            
            all_results[model_key] = result
            
            # Affichage des r√©sultats
            if result['responses']:
                avg_time = sum(result['response_times']) / len(result['response_times'])
                print(f" Temps de r√©ponse moyen: {avg_time:.3f}s")
                print(f"Nombre de tests r√©ussis: {len(result['responses'])}")
                
                # Afficher quelques exemples
                for i, resp in enumerate(result['responses'][:3]):
                    print(f"   Exemple {i+1}: '{resp['text'][:50]}...' ‚Üí {resp['sentiment']} ({resp['confidence']:.2f})")
            else:
                print("‚ùå Aucun test r√©ussi")
            
            if result['errors']:
                print(f"Erreurs: {len(result['errors'])}")
                for error in result['errors'][:2]:  # Afficher seulement les 2 premi√®res erreurs
                    print(f"   - {error}")
        
        self.results = all_results
        return all_results
    
    def generate_comparison_table(self) -> pd.DataFrame:
        """G√©n√®re un tableau comparatif des mod√®les"""
        comparison_data = []
        
        for model_key, model_info in self.models.items():
            result = self.results.get(model_key, {})
            
            avg_time = 0
            success_rate = 0
            if result.get('responses'):
                avg_time = sum(result['response_times']) / len(result['response_times'])
                success_rate = len(result['responses']) / (len(result['responses']) + len(result.get('errors', []))) * 100
            
            comparison_data.append({
                'Mod√®le': model_key,
                'Description': model_info['description'],
                'Langues': ', '.join(model_info['languages']),
                'Pr√©cision attendue': model_info['expected_accuracy'],
                'Taille (MB)': model_info['model_size_mb'],
                'Temps moyen (s)': f"{avg_time:.3f}",
                'Taux de succ√®s (%)': f"{success_rate:.1f}",
                'Type de test': result.get('type', 'N/A'),
                'Erreurs': len(result.get('errors', []))
            })
        
        return pd.DataFrame(comparison_data)
    
    def plot_results(self):
        """G√©n√®re des graphiques de comparaison"""
        if not self.results:
            print("Aucun r√©sultat √† afficher")
            return
        
        # Pr√©paration des donn√©es pour les graphiques
        models = []
        avg_times = []
        success_rates = []
        
        for model_key, result in self.results.items():
            if result.get('responses'):
                avg_time = sum(result['response_times']) / len(result['response_times'])
                success_rate = len(result['responses']) / (len(result['responses']) + len(result.get('errors', []))) * 100
                
                models.append(model_key)
                avg_times.append(avg_time)
                success_rates.append(success_rate)
        
        # Cr√©ation des graphiques
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Graphique des temps de r√©ponse
        bars1 = ax1.bar(models, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax1.set_title('Temps de r√©ponse moyen par mod√®le', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Temps (secondes)')
        ax1.tick_params(axis='x', rotation=45)
        
        # Ajout des valeurs sur les barres
        for bar, time_val in zip(bars1, avg_times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                    f'{time_val:.3f}s', ha='center', va='bottom')
        
        # Graphique des taux de succ√®s
        bars2 = ax2.bar(models, success_rates, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax2.set_title('Taux de succ√®s par mod√®le', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Taux de succ√®s (%)')
        ax2.tick_params(axis='x', rotation=45)
        
        # Ajout des valeurs sur les barres
        for bar, rate in zip(bars2, success_rates):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{rate:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("üìä Graphiques sauvegard√©s dans 'benchmark_results.png'")
    
    def save_detailed_results(self, filename: str = 'detailed_results.json'):
        """Sauvegarde les r√©sultats d√©taill√©s en JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"R√©sultats d√©taill√©s sauvegard√©s dans '{filename}'")

def main():
    """Fonction principale"""
    print("BENCHMARK D'ANALYSE DE SENTIMENTS - HUGGING FACE")
    print("=" * 60)
    print("Ce script compare 3 mod√®les d'analyse de sentiments:")
    print("1. DistilBERT (anglais)")
    print("2. BERT multilingue (5 √©toiles)")
    print("3. RoBERTa Twitter (anglais)")
    print()
    
    # Initialisation de l'analyseur
    analyzer = SentimentAnalyzer()
    
    # Choix du mode de test
    print("Choisissez le mode de test:")
    print("1. Test local (recommand√© pour la premi√®re fois)")
    print("2. Test API (gratuit, 30 requ√™tes/jour)")
    
    choice = input("Votre choix (1 ou 2): ").strip()
    use_api = choice == "2"
    
    if use_api:
        print("\nMode API s√©lectionn√© - Limite de 30 requ√™tes/jour")
        print("   Les tests peuvent √©chouer si la limite est atteinte")
    
    # Lancement du benchmark
    results = analyzer.run_benchmark(use_api=use_api)
    
    # G√©n√©ration du tableau comparatif
    print("\n" + "=" * 60)
    print("TABLEAU COMPARATIF")
    print("=" * 60)
    
    comparison_df = analyzer.generate_comparison_table()
    print(comparison_df.to_string(index=False))
    
    # Sauvegarde des r√©sultats
    analyzer.save_detailed_results()
    
    # G√©n√©ration des graphiques
    print("\n G√©n√©ration des graphiques...")
    analyzer.plot_results()
    
    # Affichage des recommandations
    print("\n" + "=" * 60)
    print("üí° RECOMMANDATIONS")
    print("=" * 60)
    
    best_time = float('inf')
    best_model = None
    
    for model_key, result in results.items():
        if result.get('responses'):
            avg_time = sum(result['response_times']) / len(result['response_times'])
            if avg_time < best_time:
                best_time = avg_time
                best_model = model_key
    
    if best_model:
        print(f"üèÜ Mod√®le le plus rapide: {best_model} ({best_time:.3f}s)")
    
    print("\nRecommandations d'usage:")
    print("- Pour l'anglais uniquement: DistilBERT (rapide et pr√©cis)")
    print("- Pour le multilingue: BERT multilingue (5 √©toiles)")
    print("- Pour les r√©seaux sociaux: RoBERTa Twitter")
    print("- Pour la production: Testez d'abord en local, puis API")

if __name__ == "__main__":
    main() 